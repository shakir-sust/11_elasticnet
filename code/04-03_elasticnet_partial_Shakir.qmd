---
title: "Elastic Net"
format: html
---

# Learning objectives  
Our learning objectives are to:  
  - Understand linear regression penalization types 
  - Use the ML framework to:  
    - pre-process data
    - train an elastic net model 
    - evaluate model predictability 

# Introduction  
As we previously learned, linear regression models can suffer from **multicollinearity** when two or more predictor variables (i.e., x variabels) are highly correlated. Elastic net is also a linear regression model.  

The methods we mentioned to overcome multicollinearity include:  
  - Dimensionality reduction (e.g., PCA)  
  - Variable selection:
    - by hand
    - by models  
    
One approach to use models to perform variable selection is through applying a **regularization** to the model in the form of **penalties**.  

In **linear regression** (which is fit with ordinary least squares-OLS, prone to multicollinearity), model coefficients are estimated by MINIMIZING the **sum of squares of the error** (SSE), which is the sum of all square distances from each observation to the regression line (observed - predicted = residual).  

![](https://bradleyboehmke.github.io/HOML/06-regularized-regression_files/figure-html/hyperplane-1.png)  

$$ minimize(SSE) $$

In **regularized linear regression**, penalties are applied on the SSE.

When penalties are applied, the coefficient of unimportant and/or correlated variables get constrained, reducing their influence in the model.  
There are three common penalty parameters we can implement:  

  - Ridge
  - Lasso (or LASSO)
  - Elastic net (or ENET), which is a combination of ridge and lasso.

## Penalties  

## Ridge penalty (L2)  
The size of this penalty, referred to as L2 (or Euclidean) norm, can take on a wide range of values, which is controlled by the **tuning parameter λ**.  

$$ minimize (SSE + L2) $$

where

$$ L2 = \lambda 
\begin{equation}
\sum_{j=1}^{p} \beta_{j}^2
\end{equation} $$  

When λ = 0, there is no effect and our objective function equals the normal ordinary least squares (OLS) regression objective function of simply minimizing SSE. 

However, as λ → ∞, the penalty becomes large and forces the coefficients toward zero (but not all the way), as in the figure below. 

![](https://bradleyboehmke.github.io/HOML/06-regularized-regression_files/figure-html/ridge-coef-example-1.png)

However, ridge regression does not perform feature selection and will **retain all available features in the final model**.   

Therefore, a ridge model is good if you believe there is a need to retain all features in your model yet reduce the noise that less influential variables may create (e.g., in smaller data sets with severe multicollinearity).   

If greater interpretation is necessary and many of the features are redundant or irrelevant then a lasso or elastic net penalty may be preferable.

## Very Important Note:

*Ridge vs. Lasso:* One key difference between Ridge compared to LASSO penalty is that Ridge does not perform variable selection (i.e., Ridge does not set any coefficients to 0; it only brings the coefficients really close to 0, for example 0.0001 will be the slope. But it will be never actually be 0). One of the benefits of using Ridge is that you sill retain all of your variables, you just force some of them to be more important than the others. But all the variable are still in your model. So, if you believe that there is a need to retain all of your x variables, but you still want to reduce the effect of multicollinearity and the noise that creates in your model, maybe Ridge is the best for you. So, it's still keeping all those x variables, none of them are excluded but some of them are losing importance because their coefficients are being shrunk.

In one sentence: LASSO can set the coefficients to 0 to do a variable selection for you, whereas Ridge never sets the coefficients completely to 0 , instead Ridge operates by bringing/shrinking some of the coefficients to be near 0 (really close to 0 e.g., 0.0001). Ridge regression does not perform feature selection and will retain all available features in the final model. 

*Difference between Ridge and LASSO in terms of λ:* The λ for L1 (LASSO) ranges from 0-1, whereas the λ for L2 (Ridge) does not have a limit, it can go to infinity. 

## Lasso penalty (L1)  
The lasso (least absolute shrinkage and selection operator) penalty is an alternative to the ridge penalty that requires only a small modification. The only difference is that we swap out the  
L2 norm for an L1 norm.  

$$ minimize (SSE + L1) $$

where

$$ L1 = \lambda 
\begin{equation}
\sum_{j=1}^{p} |\beta_{j}|
\end{equation} $$

Whereas the ridge penalty pushes variables to approximately but not equal to zero, the **lasso** penalty will actually **push coefficients all the way to zero** as in the below figure:  

![](https://bradleyboehmke.github.io/HOML/06-regularized-regression_files/figure-html/lasso-coef-example-1.png)

Switching to the lasso penalty not only improves the model but it also **conducts automated feature selection**.

##  Ridge + Lasso = Elastic net  

A generalization of the ridge and lasso penalties, called the elastic net, **combines the two penalties**.  

$$ minimize (SSE + L2 + L1) $$

Although lasso models perform feature selection, when two strongly correlated features are pushed towards zero, one may be pushed fully to zero while the other remains in the model. Furthermore, the process of one being in and one being out is not very systematic.   

In contrast, the ridge regression penalty is a little more effective in systematically handling correlated features together.  

[Note: In Elastic Net, both Ridge and LASSO work together: all the variables are shrunk close to 0 by Ridge, then set to 0 by LASSO.]

Consequently, the advantage of the elastic net penalty is that it **enables effective regularization** via the ridge penalty with the **feature selection** characteristics of the lasso penalty, as in the figure below:    

![](https://bradleyboehmke.github.io/HOML/06-regularized-regression_files/figure-html/elastic-net-coef-example-1.png)

# Setup  

```{r}
#| message: false
#| warning: false

#install.packages("glmnet") #"glmnet" package is used to implement elastic net
#install.packages("vip") #"vip" package is used to look into variable importance across different machine learning models
#install.packages("tidymodels") #"tidymodels" is a family of packages developed by tideverse that help us implement machine learning workflow in R, where each package have different roles in this workflow

library(tidymodels) #family of packages for ML workflow in R
library(tidyverse)
library(glmnet) #to implement elastic net
library(vip)

```

```{r weather}

weather <- read_csv("../data/weather_monthsum.csv")

weather
```

# ML workflow  
Let's use the workflow defined in the lecture below.  
In R, we will use many packages built specifically for different steps on this workflow, all of which use tidyverse principles.    

These packages are made available as a bundle through the meta-package `tidymodels` (https://www.tidymodels.org/packages/), and include:  
  - `rsamples` for data split and resampling  
  - `recipes` for data processing  
  - `parsnip` to specify model types and engines  
  - `tune` to fine-tune hyper-parameters  
  - `dials` to create grids  
  - `yardstick` to assess performance  

## 1. Pre-processing  
Here's where we perform **data split** and **data processing**.  

### a. Data split  
For data split, let's use **70% training / 30% testing**.

```{r weather_split}

# Setting seed to get reproducible results  
set.seed(931735) #setting seed to make sure that we get reproducible results

# Setting split level  
weather_split <- initial_split(weather, 
                               prop = .7, #to specify 70% training/ 30% testing #if we specified prop = .8, it would represent 80% trainin/ 20% testing
                               strata = strength_gtex #to do a stratified sampling based on our predicted variable "strength_gtex"; if we do not do this, by random chance we can have
                               )

weather_split

```



```{r weather_train}

# Setting train set 
weather_train <- training(weather_split) #"training()" function is from rsamples package; inside the "training()" function, we specify the split dataset "weather_split"

weather_train #weather_train" is the training data frame

```

How many observations?

```{r weather_test}

# Setting test split
weather_test <- testing(weather_split) #inside the "testing()" function, we specify the split dataset "weather_split"

weather_test

```
How many observations?  


[Very important note: Data shift can happen due to many different reasons. One of the ways to check for data shift is to check the distribution of our testing and training data, especially related to our predicted variable, in this case "strength_gtex".]

Let's check the distribution of our predicted variable **strength_gtex** across training and testing (to check for data shift):  

```{r distribution}

ggplot() +
  geom_density(data = weather_train, 
               aes(x = strength_gtex),
               color = "red") + #density plot for the training set
  geom_density(data = weather_test, 
               aes(x = strength_gtex),
               color = "blue") #density plot for the testing set
  

```

We see that the distribution of our predicted variable "strength_gtex" across our training and testing data is almost similar with no strong mismatch (i.e., it's not bad, it could have been worse) given the fact that we did a completely random sample in which 70% went to the training and 30% went to the testing. If there was a more mismatch (i.e,. not a good agreement) between the distribution of the training and testing data, it would indicate a data shift which we could fix by conducting stratified sampling based on our predicted variable "strength_gtex" [need to specify "strata = strength_gtex" inside the "initial_split()" function; full code: weather_split <- initial_split(weather, prop = .7, strata = strength_gtex) ]. We can specify any variable inside "strata = " argument, it does not always has to be the y/predicted/response variable

If we did not do stratified sampling, it could happen that just by random chance we could get really low values of "strength_gtex" in our testing set and really medium and high values of "strength_gtex" in our training set, which is data shift (in which we have a different data distribution of our predicted variable across training and testing sets which we do not want to have; what we want to have is a similar data distribution of the predicted variable so that we can make a model that can predict unseen data, which we can obtain by doing a stratified sampling on our predicted variable "strength_gtex") 

Let's go back to the split chunk and use **stratified sampling** based on our predicted variable **strength_gtex**.

Now, we put our **test set** aside and continue with our **train set** for training.  

### b. Data processing  
Before training, we need to perform some processing steps, like  
  - **removing unimportant variables**  
  - **performing PCA on the go**    
  - normalizing  
  - dropping NAs  
  - removing columns with single value  
  - others?  

For that, we'll create a **recipe** of these processing steps. ["recipe" is a term which is very specific to "tidymodels"] 

This recipe will then be applied now to the **train data**, and easily applied to the **test data** when we bring it back at the end.

Creating a recipe is as easy way to port your (pre-)processing steps for other data sets without needing to repeat code, and also only considering the data it is being applied to.  

Different model types require different processing steps.  
Let's check what steps are required for an elastic net model (linear_reg).
We can search for that in this link: https://www.tmwr.org/pre-proc-table  

In our case, the elastic net model type that we are gonna run is "linear_reg" (linear regressinon). From Table A.1 (Preprocessing methods for different models) of the chapter in the e-book (link mentioned above: https://www.tmwr.org/pre-proc-table), the recommended pre-processing steps for "linear_reg" model are dummy (creating dummy variables, this applies when we have categorical predictor variables; in our case we do not have any categorical variables as all of our predictor variables are numerical, so we don't have to do that in our case), zv (stands for zero variance; this is required when we have a column as a predictor that has only 1 value: for example, there is a column that only contains the number 1 from top of the column to all the way down, so there is 0 variance in the column as all entry is the same i.e., number 1. We don't have that in our data, so we don't have to worry about applying the zv [zero variance] step). Next, the table also recommends to apply some sort of imputation steps if we have NA (i.e., missing values) in any of our columns. We do not have any NAs in any of our columns, so we don't have to do the "impute" step because we don't have any NAs. Next, the table recommends to do "decorrelate" which is done through PCA. We will see how to do if even though I'm gonna ask we don't do it here because of a specific reason that we will see in the end, but it is recommending to do a "decorrelate" through PCA if we are using "linear_reg". All these steps are model specific and changes from model to model.    

You can find all available recipe step options here: https://tidymodels.github.io/recipes/reference/index.html


```{r weather_recipe}
weather_recipe <-
  # Defining predicted and predictor variables
  recipe(strength_gtex ~ ., # . is used to indicate all predictor variables should be included in the model (we have 73 predictor variables in this data set)
          data = weather_train) %>%  #note that we are using the training data in this recipe
  # Removing year, site, and weather outside of growing season  
  step_rm(year, site, #although we could have used dplyr::select, but we used step_rm() because it is within the scope of the recipe
          matches("Jan|Feb|Mar|Apr|Nov|Dec") #to remove all columns that have these months as part of their names
          )
  # Decorrelating
  #step_pca(all_numeric(), -all_outcomes(), num_comp = 7) #"num_comp = 7": to keep 7 PCs because we need 7 PCs to explain 80% total variance in this case (but it could be any number up to 72)

```

Now that we have our recipe ready, we **need to apply it** to the training data in a process called prepping:

```{r weather_prep}

weather_prep <- weather_recipe %>%
  prep()

weather_prep #Trained data

```


Now, we're ready to start the model training process!

## 2. Training  
### a. Model specification  
First, let's specify:  
  - the **type of model** we want to train  
  - which **engine** we want to use  

An elastic net model is a linear regression model, penalized.  

Elastic net **hyperparameters**:  
  - **penalty**: equivalent to lambda  
  - **mixture**: 0 (ridge) to 1 (lasso) [can be anywhere from 0 to 1]   

Let's create a model specification that will **fine-tune** these for us.

A given model type can be fit with different engines (e.g., through different packages). Here, we'll use the **glmnet** engine/package.  
  
```{r enet_spec}
enet_spec  <- 
  # Specifying linear regression as our model type, asking to tune the hyperparameters
  linear_reg(penalty = tune(),
             mixture = tune()
             ) %>%
  # Specify the engine
  set_engine("glmnet")


enet_spec
```
Notice how the main arguments above do not have a value **yet**, because they will be tuned.  

### b. Hyper-parameter tuning  
Now, let's create a **grid** to perform our hyperparameter tuninig search with multiple combinations of values for penalty and mixture:

```{r enet_grid}
enet_grid <- crossing(penalty = seq(0, 
                                    10, 
                                    by = 1),
                      mixture = seq(0, 
                                    1, 
                                    by = 0.1))

enet_grid
```

Our grid has **121** potential combinations of mixture and penalty.

```{r}

ggplot(data = enet_grid,
       aes(x = mixture,
           y = penalty)
       ) +
  geom_point()

```

For our grid search, we need:  
  - Our model specification (`enet_spec`)  
  - The recipe (`weather_recipe`)  
  - The grid (`enet_grid`), and    
  - Our **resampling strategy** (don't have yet)  
  
Let's define our resampling strategy below, using a 10-fold cross validation approach:  

```{r resampling_foldcv}

resampling_foldcv <- vfold_cv(weather_train,
                              v = 10)

resampling_foldcv

resampling_foldcv$splits[[1]] #On the 1st fold, we have 438 for training, and 49 for assessment
#resampling_foldcv$splits[[2]]

```

On each fold, we'll use **437** observations for training and **49** observations to assess performance.    

Now, let's perform the grid search below:  

```{r enet_grid_result}

enet_grid_result <- tune_grid(enet_spec,
                              preprocessor = weather_recipe,
                              grid = enet_grid,
                              resamples = resampling_foldcv
                              )

enet_grid_result
enet_grid_result$.metrics[[1]]

```

Why 242 rows?  

121 (each a combination of mixture and penalty) x 2 (2 assessment metrics: rmse, rsq)  

Let's collect a summary of metrics (across all folds, for each mixture x penalty combination), and plot them.  

Firs, RMSE (lower is better):  

```{r RMSE}

enet_grid_result %>%
  collect_metrics() %>% #this line will come in exam as fill in the blanks 
  filter(.metric == "rmse") %>%
  ggplot(aes(x = penalty, 
             y = mean, 
             color = factor(mixture), 
             group = factor(mixture))) +
  geom_line() +
  geom_point() + 
  labs(y = "RMSE")
  
```

What penalty and mixture values created lowest RMSE?  rewatch video

Now, let's look into R2 (higher is better):  

```{r R2}
enet_grid_result %>%
  collect_metrics() %>%
  filter(.metric == "rsq") %>%
  ggplot(aes(x = penalty, 
             y = mean, 
             color = factor(mixture), 
             group = factor(mixture))) +
  geom_line() +
  geom_point() + 
  labs(y = "R2")
```

What penalty and mixture values created lowest RMSE?  
It seems that our best model is with hyperparameters set to:  
  - mixture = 0  
  - penalty = 0  
  
Let's extract the hyperparameters from the best model as judged by 2 performance metrics:  
```{r}

# Based on lowest RMSE
best_rmse <- enet_grid_result %>%
  select_best(metric = "rmse")

best_rmse

```

```{r}

# Based on greatest R2
best_r2 <- enet_grid_result %>%
  select_best(metric = "rsq")

best_r2

```
Based on RMSE, we would choose mixture = 0, penalty = 1.

Based on R2, we would choose mixture = 0.1, penalty = 1.

Let's use the hyperparameter values that optimized R2 to fit our final model.

```{r final_spec}

final_spec <- linear_reg(penalty = 0.7,
                         mixture = 0) %>%
  set_engine("glmnet")

final_spec

```

## 3. Validation  
Now that we determined our best model, let's do our **last fit**.

This means 2 things:  
  - Traninig the optimum hyperparameter values on the **entire training set**  
  - Using it to **predict** on the **test set**  

These 2 steps can be completed in one function, as below:  

```{r final_fit}

final_fit <- last_fit(final_spec,
                      weather_recipe,
                      split = weather_split 
                      )

final_fit %>%
  collect_predictions()


```
Why 212 observations?  

Metrics on the **test set**:
```{r}

final_fit %>%
  collect_metrics()

```

Metrics on **train set** (for curiosity and compare to test set):  
```{r}
# RMSE
final_spec %>%
  fit(strength_gtex ~ .,
      data = bake(weather_prep, 
                  weather_train)) %>%
  augment(new_data = bake(weather_prep, 
                          weather_train)) %>% 
  rmse(strength_gtex, .pred) %>%
  bind_rows(
    
    
    # R2
    final_spec %>%
      fit(strength_gtex ~ .,
          data = bake(weather_prep, 
                      weather_train)) %>%
      augment(new_data = bake(weather_prep, 
                              weather_train)) %>% 
      rsq(strength_gtex, .pred)
    
  )

```
How does metrics on test compare to metrics on train?  

Why?

Predicted vs. observed plot:  
```{r}
final_fit %>%
  collect_predictions() %>%  #will come in exam
  ggplot(aes(x = strength_gtex, #will come in exam
             y = .pred)) + #will come in exam
  geom_point() +
  geom_abline() + #will come in exam
  geom_smooth(method = "lm") +
  scale_x_continuous(limits = c(20, 40)) +
  scale_y_continuous(limits = c(20, 40)) 
```

We always want to see this plot in Machine Learning.

Coefficients:  
```{r}
final_spec %>%
  fit(strength_gtex ~ .,
         data = bake(weather_prep, weather)) %>%
  tidy() %>%
  arrange(desc(estimate))

```

Variable importance:  

```{r}

final_spec %>%
  fit(strength_gtex ~ .,
         data = bake(weather_prep, weather)) %>%
    vi() %>%  #will come in exam
  mutate(
    Importance = abs(Importance),
    Variable = fct_reorder(Variable, 
                           Importance)
  ) %>%
  slice_head(n = 10) %>%
  ggplot(aes(x = Importance, 
             y = Variable, 
             fill = Sign)) +
  geom_col() +
  scale_x_continuous(expand = c(0, 0)) +
  labs(y = NULL)
    
```

**Therefore, tmin in September (positive effect), and tmax in September (negative effect) were the most important variables affecting cotton fiber strength.**  

# Summary  
In this exercise, we covered: 
  - Penalized linear regression types (ridge, lasso, elastic net)  
  - Set up a ML workflow to train an elastic net model  
  - Used `recipes` to process data
  - Used `rsamples` to split data  
  - Used a fixed grid to search the best values for mixture and penalty  
  - Used 10-fold cross validation as the resampling method  
  - Used both R2 and RMSE as the metrics to select best model  
  - Once final model was determined, used it to predict **test set**  
  - Evaluated it with predicted vs. observed plot, R2 and RMSE metrics, and variable importance  
  

