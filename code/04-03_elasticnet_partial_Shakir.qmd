---
title: "Elastic Net"
format: html
---

Very Important Note:

**To learn any Machine Learning model, we need to know:**
- what the model does
- how the model works
- what key hyperparamters it has


# Learning objectives  
Our learning objectives are to:  
  - Understand linear regression penalization types 
  - Use the ML framework to:  
    - pre-process data
    - train an elastic net model 
    - evaluate model predictability 

# Introduction  
As we previously learned, linear regression models can suffer from **multicollinearity** when two or more predictor variables (i.e., x variabels) are highly correlated. *Elastic net is also a linear regression model.*  

The methods we mentioned to overcome multicollinearity include:  
  - Dimensionality reduction (e.g., PCA)  
  - Variable selection:
    - by hand
    - by models  
    
One approach to use models to perform variable selection is through applying a **regularization** to the model in the form of **penalties**.  

In **linear regression** (which is fit with ordinary least squares-OLS, prone to multicollinearity), model coefficients are estimated by MINIMIZING the **sum of squares of the error** (SSE), which is the sum of all square distances from each observation to the regression line (observed - predicted = residual).  

Note: The blue line is the best fit line that minimizes the sum of squares of error (SSE)

![](https://bradleyboehmke.github.io/HOML/06-regularized-regression_files/figure-html/hyperplane-1.png)  

$$ minimize(SSE) $$

For every regression model, what we are trying to optimize is to minimize the Sum of Squares of Error (SSE).   

In **regularized linear regression**, penalties are applied on the SSE.

When penalties are applied, the coefficient of unimportant and/or correlated variables get constrained, reducing their influence in the model.  
There are three common penalty parameters we can implement:  

  - Ridge
  - Lasso (or LASSO)
  - Elastic net (or ENET), which is a combination of ridge and lasso.

## Penalties  

## Ridge penalty (L2)  
The size of this penalty, referred to as L2 (or Euclidean) norm, can take on a wide range of values, which is controlled by the **tuning parameter λ**.  

$$ minimize (SSE + L2) $$

where

$$ L2 = \lambda 
\begin{equation}
\sum_{j=1}^{p} \beta_{j}^2
\end{equation} $$  

When λ = 0, there is no effect and our objective function equals the normal ordinary least squares (OLS) regression objective function of simply minimizing SSE. 

However, as λ → ∞, the penalty becomes large and forces the coefficients toward zero (but not all the way), as in the figure below. 

[Note: Beta j is gonna be the different slopes that you will have for for different variables. Imagine that we have a model in which fiber strength (y) is explained by all 74 weather variables (74 x variables/ 74 predictor variables), each one of these x variables will have a beta which is the slope for each of the predictor (x) variables]

![](https://bradleyboehmke.github.io/HOML/06-regularized-regression_files/figure-html/ridge-coef-example-1.png)

[Note: In the above graph, coefficients = slope values]

However, ridge regression does not perform feature selection and will **retain all available features in the final model**.   

Therefore, a ridge model is good if you believe there is a need to retain all features in your model yet reduce the noise that less influential variables may create (e.g., in smaller data sets with severe multicollinearity).   

If greater interpretation is necessary and many of the features are redundant or irrelevant, then a lasso or elastic net penalty may be preferable.

## Very Important Note:

*Ridge vs. Lasso:* One key difference between Ridge compared to LASSO penalty is that Ridge does not perform variable selection (i.e., Ridge does not set any coefficients to 0; it only brings the coefficients really close to 0, for example 0.0001 will be the slope. But it will be never actually be 0). One of the benefits of using Ridge is that you sill retain all of your variables, you just force some of them to be more important than the others. But all the variable are still in your model. So, if you believe that there is a need to retain all of your x variables, but you still want to reduce the effect of multicollinearity and the noise that creates in your model, maybe Ridge is the best for you. So, it's still keeping all those x variables, none of them are excluded but some of them are losing importance because their coefficients are being shrunk.

However, if you want better interpretation out of the model and you know that some features are redundant or irrelevant and you don't need all of them, using LASSO or ealstic net (ENET: combines Ridge and LASSO) will be preferable.

In one sentence: LASSO can set the coefficients to 0 to do a variable selection for you, whereas Ridge never sets the coefficients completely to 0 , instead Ridge operates by bringing/shrinking some of the coefficients to be near 0 (really close to 0 e.g., 0.0001). Ridge regression does not perform feature selection and will retain all available features in the final model. 

*Difference between Ridge and LASSO in terms of λ:* The λ for L1 (LASSO) ranges from 0-1, whereas the λ for L2 (Ridge) does not have a limit, it can go to infinity. 

## Lasso penalty (L1)  
The LASSO (least absolute shrinkage and selection operator) penalty is an alternative to the ridge penalty that requires only a small modification. The only difference is that we swap out the  
L2 norm for an L1 norm.  

$$ minimize (SSE + L1) $$

where

$$ L1 = \lambda 
\begin{equation}
\sum_{j=1}^{p} |\beta_{j}|
\end{equation} $$

Whereas the ridge penalty pushes variables to approximately but not equal to zero, the **lasso** penalty will actually **push coefficients all the way to zero** as in the below figure:  

![](https://bradleyboehmke.github.io/HOML/06-regularized-regression_files/figure-html/lasso-coef-example-1.png)

Switching to the lasso penalty (L1) not only improves the model but it also **conducts automated feature selection**.

##  Ridge + Lasso = Elastic net  

A generalization of the ridge and lasso penalties, called the elastic net, **combines the two penalties**.  

$$ minimize (SSE + L2 + L1) $$

Although lasso models perform feature selection, when two strongly correlated features (= x variables) are pushed towards zero, one may be pushed fully to zero while the other remains in the model. Furthermore, the process of one being in and one being out is not very systematic.   

In contrast, the ridge regression penalty is a little more effective in systematically handling correlated features together.  

[Note: In Elastic Net, both Ridge and LASSO work together: all the variables are shrunk close to 0 by Ridge, then set to 0 by LASSO.]

Consequently, the advantage of the elastic net penalty is that it **enables effective regularization** via the ridge penalty with the **feature selection** characteristics of the lasso penalty, as in the figure below:    

![](https://bradleyboehmke.github.io/HOML/06-regularized-regression_files/figure-html/elastic-net-coef-example-1.png)

# Setup  

```{r}
#| message: false
#| warning: false

#install.packages("glmnet") #"glmnet" package is used to implement elastic net
#install.packages("vip") #"vip" package is used to look into variable importance across different machine learning models
#install.packages("tidymodels") #"tidymodels" is a family of packages developed by tideverse that help us implement machine learning workflow in R, where each package have different roles in this workflow

library(tidymodels) #family of packages for ML workflow in R
library(tidyverse)
library(glmnet) #to implement elastic net
library(vip)

```

```{r weather}

weather <- read_csv("../data/weather_monthsum.csv")

weather
```

# ML workflow  
Let's use the workflow defined in the lecture below.  
In R, we will use many packages built specifically for different steps on this workflow, all of which use tidyverse principles.    

These packages are made available as a bundle through the meta-package `tidymodels` (https://www.tidymodels.org/packages/), and include:  
  - `rsamples` for data split and resampling  
  - `recipes` for data processing  
  - `parsnip` to specify model types and engines  
  - `tune` to fine-tune hyper-parameters  
  - `dials` to create grids  
  - `yardstick` to assess performance  

## 1. Pre-processing  
Here's where we perform **data split** and **data processing**.  

### a. Data split  
For data split, let's use **70% training / 30% testing**.

```{r weather_split}

# Setting seed to get reproducible results  
set.seed(931735) #setting seed to make sure that we get reproducible results

# Setting split level  
weather_split <- initial_split(weather, 
                               prop = .7, #to specify the proportion that goes into training: 0.7 for 70% training/ 30% testing #if we specified prop = .8, it would represent 80% training/ 20% testing
                               strata = strength_gtex #strata = strength_gtex: to do a stratified sampling based on our predicted (y) variable "strength_gtex"; how stratified sampling works: imagine we have a distribution of our entire 698 rows of "strength_gtex", it's basically creating bins of quantiles across the distribution and then doing the 70%/30% split within each bin to make sure that same proportion of different bins within our distribution are going to training and test. If we do not do stratified sampling, it could happen that just by random chance we get really low values of strength in our testing, and really medium or high values in our training, which is data shift (when we have a different data distribution of our predicted variable across training and test) and we do NOT want data shift. We want our training and testing to have similar data distribution for our predicted variable, so we can make sure we create a model that can predict unseen data. So, doing stratified sampling on our predicted variable "strength_gtex", we avoid data shift.
                               )

weather_split

```
The output does not look like a data frame. It's just telling us what's gonna happen with our data. It's telling us that we have a total of 698 rows, and 487 of these rows are gonna go to training, and 210 is going to testing.

Next, in the following 2 chunks, we are gonna retrieve those 487 rows in training set, and 211 in testing set, then assign those data frames to their own objects. 

*Very important note on data shift and stratified sampling:* 

**weather_split <- initial_split(weather, prop = .7, strata = strength_gtex)**: to do a stratified sampling based on our predicted (y) variable "strength_gtex"; how stratified sampling works: imagine we have a distribution of our entire 698 rows of "strength_gtex", it's basically creating bins of quantiles across the distribution and then doing the 70%/30% split within each bin to make sure that same proportion of different bins within our distribution are going to training and test. If we do not do stratified sampling, it could happen that just by random chance we get really low values of strength in our testing, and really medium or high values in our training, which is data shift (when we have a different data distribution of our predicted variable across training and test) and we do NOT want data shift. We want our training and testing to have similar data distribution for our predicted variable, so we can make sure we create a model that can predict unseen data. So, doing stratified sampling on our predicted variable "strength_gtex", we avoid data shift.


```{r weather_train}

# Setting train set 
weather_train <- training(weather_split) #"training()" function is from rsamples package; inside the "training()" function, we specify the split dataset "weather_split"

weather_train #weather_train" is the object containing training set/ data frame

```

How many observations? 487 [same as <*Training*/Testing/Total> = <*487*/211/698>]

```{r weather_test}

# Setting test split
weather_test <- testing(weather_split) #inside the "testing()" function, we specify the split dataset "weather_split"

weather_test  #weather_test" is the object containing testing set/ data frame

```
How many observations?  211 [same as <Training/*Testing*/Total> = <487/*211*/698>]

[Very important note: Data shift can happen due to many different reasons (e.g., due to randomness). One of the ways to check for data shift is to check the distribution of our testing and training data, especially related to our predicted variable, in this case "strength_gtex".]

Let's check the distribution of our predicted variable **strength_gtex** across training and testing (to check for data shift):  

```{r distribution}

ggplot() +
  geom_density(data = weather_train, 
               aes(x = strength_gtex),
               color = "red") + #density plot for the training set
  geom_density(data = weather_test, 
               aes(x = strength_gtex),
               color = "blue") #density plot for the testing set
  

```

We see that the distribution of our predicted variable "strength_gtex" across our training and testing data is almost similar with no strong mismatch (i.e., it's not bad, it could have been worse) given the fact that we did a completely random sample in which 70% went to the training and 30% went to the testing. If there was a more mismatch (i.e,. not a good agreement) between the distribution of the training and testing data, it would indicate a data shift which we could fix by conducting stratified sampling based on our predicted variable "strength_gtex" [need to specify "strata = strength_gtex" inside the "initial_split()" function; full code: weather_split <- initial_split(weather, prop = .7, strata = strength_gtex) ]. We can specify any variable inside "strata = " argument, it does not always has to be the y/predicted/response variable

If we did not do stratified sampling, it could happen that just by random chance we could get really low values of "strength_gtex" in our testing set and really medium and high values of "strength_gtex" in our training set, which is data shift (in which we have a different data distribution of our predicted variable across training and testing sets which we do not want to have; what we want to have is a similar data distribution of the predicted variable so that we can make a model that can predict unseen data, which we can obtain by doing a stratified sampling on our predicted variable "strength_gtex") 

*Very important note on data shift and stratified sampling:* 

**weather_split <- initial_split(weather, prop = .7, strata = strength_gtex)**: to do a stratified sampling based on our predicted (y) variable "strength_gtex"; how stratified sampling works: imagine we have a distribution of our entire 698 rows of "strength_gtex", it's basically creating bins of quantiles across the distribution and then doing the 70%/30% split within each bin to make sure that same proportion of different bins within our distribution are going to training and test. If we do not do stratified sampling, it could happen that just by random chance we get really low values of strength in our testing, and really medium or high values in our training, which is data shift (when we have a different data distribution of our predicted variable across training and test) and we do NOT want data shift. We want our training and testing to have similar data distribution for our predicted variable, so we can make sure we create a model that can predict unseen data. So, doing stratified sampling on our predicted variable "strength_gtex", we avoid data shift.

Let's go back to the split chunk and use **stratified sampling** based on our predicted variable **strength_gtex**.

Now, we put our **test set** aside and continue with our **train set** for training.  

### b. Data processing  
Before training, we need to perform some processing steps, like  
  - **removing unimportant variables**  
  - **performing PCA on the go**    
  - normalizing  
  - dropping NAs  
  - removing columns with single value  
  - others?  

For that, we'll create a **recipe** of these processing steps. ["recipe" is a term which is very specific to "tidymodels"] 

This recipe will then be applied now to the **train data**, and easily applied to the **test data** when we bring it back at the end.

Creating a recipe is as easy way to port your (pre-)processing steps for other data sets without needing to repeat code, and also only considering the data it is being applied to.  

Different model types require different processing steps.  
Let's check what steps are required for an elastic net model i.e., linear_reg().
We can search for that in this link: https://www.tmwr.org/pre-proc-table  

In our case, the elastic net model type that we are gonna run is "linear_reg" (linear regressinon). From Table A.1 (Preprocessing methods for different models) of the chapter in the e-book (link mentioned above: https://www.tmwr.org/pre-proc-table), the recommended pre-processing steps for "linear_reg" model are dummy (creating dummy variables, this applies when we have categorical predictor variables; in our case we do not have any categorical variables as all of our predictor variables are numerical, so we don't have to do that in our case), zv (stands for zero variance; this is required when we have a column as a predictor that has only 1 value: for example, there is a column that only contains the number 1 from top of the column to all the way down, so there is 0 variance in the column as all entry is the same i.e., number 1. We don't have that in our data, so we don't have to worry about applying the zv [zero variance] step). Next, the table also recommends to apply some sort of imputation steps if we have NA (i.e., missing values) in any of our columns. We do not have any NAs in any of our columns, so we don't have to do the "impute" step because we don't have any NAs. Next, the table recommends to do "decorrelate" which is done through PCA. We will see how to do if even though I'm gonna ask we don't do it here because of a specific reason that we will see in the end, but it is recommending to do a "decorrelate" through PCA if we are using "linear_reg". All these steps are model specific and changes from model to model.    

You can find all available recipe step options here: https://tidymodels.github.io/recipes/reference/index.html


```{r weather_recipe}

weather_recipe <-
  # Defining predicted and predictor variables
  recipe(strength_gtex ~ ., # . is used to indicate all predictor variables should be included in the model (we have 73 predictor variables in this data set) #everything on the left of ~ is/are outcome(s), and everything in the right side of ~ are predictors
          data = weather_train) %>%  #note that we are using the training data in this recipe
  # Removing year, site, and weather outside of growing season  
  step_rm(year, site, #rm in step_rm() stands for remove #although we could have used dplyr::select, but we used step_rm() because it is within the scope of "recipes" #we want to remove year and site from our training set
          matches("Jan|Feb|Mar|Apr|Nov|Dec") #to remove weather variables that represent data outside of the growing season: Jan, Feb, Mar, Apr, Nov, Dec  #matches("Jan|Feb|Mar|Apr|Nov|Dec"): to remove all columns that have these months (Jan, Feb, Mar, Apr, Nov, Dec) as part of their (column) names
          )
  # Decorrelating
  #step_pca(all_numeric(), -all_outcomes(), num_comp = 7) #step_pca(all_numeric(), -all_outcomes(), num_comp = 7): apply PCA on everything that are numeric except the outcome (= strength_gtex), and keep 7 PCA components  #"num_comp = 7": to keep 7 PCs because we need 7 PCs to explain 80% total variance in this case (but it could be any number up to 72) 

weather_recipe

```

Now that we have our recipe ready, we **need to apply it** to the training data in a process called prepping:

```{r weather_prep}

weather_prep <- weather_recipe %>%
  prep() #prep(): to prepare the recipe

weather_prep #Trained data

```
On the output, notice that now it says "Trained" at the very last of the output, which indicates that the trained data is ready for further training.  

Now, we're ready to start the model training process!

## 2. Training  
### a. Model specification  
First, let's specify:  
  - the **type of model** we want to train [e.g., elastic net, Conditional inference tree, random forest etc.]  
  - which **engine** we want to use ["engine" is the package that we specify to implement random forest] 

An elastic net model is a linear regression model, penalized.  

Elastic net **hyperparameters**:  
  - **penalty**: equivalent to lambda  
  - **mixture**: 0 to 1 [how much mixture you are bringing from ridge and LASSO into your model; "mixture" can be anywhere in between 0 to 1; if mixture = 0, then it's ridge; if mixture = 1, then it's LASSO]   

Let's create a model specification that will **fine-tune** these for us [i.e., controlling for model complexity].

A given model type can be fit with different engines (e.g., through different packages). Here, we'll use the **glmnet** engine/package.  
  
```{r enet_spec}
enet_spec  <- 
  # Specifying linear regression as our model type, asking to tune the hyperparameters penalty and mixture
  linear_reg(penalty = tune(), #"tune()" function fine-tunes "penalty" and "mixture" hyperparameters for us
             mixture = tune() #at this stage, we don't specify any number in the tune() function
             ) %>% #"linear_reg()" is the overall model type #thisfunction comes from "parsenip" package
  # Specify the engine
  set_engine("glmnet") #set_engine("glmnet"): The engine here specifies which package (glmnet, in this case) we specifically want to use to apply the "linear_reg" model type #Engine = specific package that we want to use to model the model type


enet_spec
```
Notice how the main arguments above do not have a value **yet**, because they will be tuned.  

In the output, 
"Linear Regression Model Specification (regression)": here we see "regression" because out y variable is numerical; if our y variable were categorcal, then it would say "classification". 

### b. Hyper-parameter tuning  

[Note: There are 2 ways of doing fine-tuning: one is to create a fixed grid, the other one is to create a sequential search of a given grid. Today we are gonna implement the fixed grid approach.]

Now, let's create a **grid** to perform our hyperparameter tuninig search with multiple combinations of values for penalty and mixture:

```{r enet_grid}
enet_grid <- crossing(penalty = seq(0, 
                                    10, 
                                    by = 1), 
                      mixture = seq(0, 
                                    1, 
                                    by = 0.1))

#seq(0, 10, by = 1): gives a sequence [for both "penalty" and "mixture"] starting at 0 and then going till 10 in increments of 1: 0  1  2  3  4  5  6  7  8  9 10
#"crossing()" function: to get all potential combinations for all those 10 values for "penalty" and "mixture"; there are 121 combinations for all the values that we created above for "penalty" and "mixture"

enet_grid
```

Our grid has **121** potential combinations of mixture and penalty.

```{r}

ggplot(data = enet_grid,
       aes(x = mixture, #mixture = column of the "enet_grid" dataframe
           y = penalty) #penalty = column of the "enet_grid" dataframe
       ) +
  geom_point()

```

For our grid search, we need:  
  - Our model specification (`enet_spec`)  
  - The recipe (`weather_recipe`)  
  - The grid (`enet_grid`), and    
  - Our **resampling strategy** (don't have yet)  
  
[Note: All the data are split into training set and testing set (we don't touch the test set until the very end). Then, on the training set, we do resampling strategies to better understand what hyperparameter values are better for our training set. Our training set is now further gonna split into different folds (e.g., v-fold) of a given size and each one of these folds are gonna be trained at each point of the grid, and in the end we are gonna ask for R2 and RMSE on the assessment set of the training set for each one of the folds.]

Let's define our resampling strategy (only on the training set) below, using a 10-fold cross validation approach:  

```{r resampling_foldcv}

resampling_foldcv <- vfold_cv(weather_train, #we are giving only the training set (not the whole dataset)
                              v = 10 #v = 10: number of folds that we wanna do #10 is a prettey common number of folds
                              )

resampling_foldcv

resampling_foldcv$splits[[1]] #[From the output] On the 1st fold, we have 438 for training, and 49 for assessment
#resampling_foldcv$splits[[2]]

```

On each fold, we'll use **437** observations for training and **49** observations to assess performance.    

On the output, we have 10 rows because we asked for 10 folds (if we asked for a different number of folds, we would get that number of rows). In a 10 fold cross-validation, we take our training data -- 90% of our rows are gonna be used to train the model -- that means applying that 90% of the data of our training set to each of the combinations on the grid, and then getting the R2 and RMSE based on the assessment set which is the other part of the fold (the 10% remaining that we did not use for training). 

If we pull out the "splits" of the 1st fold ( resampling_foldcv$splits[[1]] ), we see that "<Analysis/Assess/Total> = <438/49/487>" there are a total of 487 observations (because that's the size of our training set), then for the training component of this 1st fold, it has 438 of those observations and 49 for the assessment (these 49 observations are gonna be used for the R2 and RMSE of the predicted by observed to give us the best model).  

If we pull out the "splits" of the 2nd fold ( resampling_foldcv$splits[[2]] ), it has the same numbers for <Analysis/Assess/Total> = <438/49/487> data as the 1st fold, but it's gonna be slightly different data that goes into the training and assessment set because they are switching the folds. So, in each one of these iterations, one of the folds are kept for assessment and all the other folds are used for training. But then the one that was used for assessment for the 1st fold goes back and a different one comes out on each one of the folds. 


Now, let's perform the grid search below:  

```{r enet_grid_result}

enet_grid_result <- tune_grid(enet_spec, #enet_spec: the model where we set out model type
                              preprocessor = weather_recipe, #we specify the object for recipe under "preprocessor =" argument
                              grid = enet_grid, #we specify the object that has the grid that we wanna evaluate
                              resamples = resampling_foldcv #we specify the object that has resampling strategy 
                              )


#it will take some time to run "enet_grid_result", because it is evaluating 121 combinations of hyperparameters and each one of these combinations are being evaluated 10 times. So, it is basically running (121 x 10 =) 1,210 models. It's going to each point of the grid, evaluating each one of them 10 times, one for each fold, and then creating R2 and RMSE out of it. So, it will take a couple of minutes to run it.

enet_grid_result #In the output, we still have our 10 folds and "splits" for each fold, and aditinally we have ".metrics" and ".notes"  

enet_grid_result$.metrics[[1]] #Let's take a look into the ".metrics" of the 1st fold. In the 1st .metric of the 1st fold, we have 242 rows and 6 columns. There is a column of penalty, mixture, .metric (indicates rmse and rsq), .estimator, .estimate, and .config. So, it's basically tested the 1st fold (that had 90% for the training to train and 10% to assess) across all of our potential grid values (for each combination of penalty and mixture), and each time it was testing each one of those models, it was then extracting the ".estimate" for the fit metrics (rmse, rsq) for us to predict the assessment set of this fold. So, it used the training part of the fold to train and the assessment part to produce the fit metrics (rmse, rsq)

```

Let's take a look into the ".metrics" of the 1st fold ( enet_grid_result$.metrics[[1]] ). In the 1st .metric of the 1st fold, we have 242 rows and 6 columns. There is a column of penalty, mixture, .metric (indicates rmse and rsq), .estimator, .estimate, and .config. So, it's basically tested the 1st fold (that had 90% for the training to train and 10% to assess) across all of our potential grid values (for each combination of penalty and mixture), and each time it was testing each one of those models, it was then extracting the ".estimate" for the fit metrics (rmse, rsq) for us to predict the assessment set of this fold. So, it used the training part of the fold to train and the assessment part to produce the fit metrics (rmse, rsq)

Why 242 rows?  

121 (each a combination of mixture and penalty) x 2 (2 assessment metrics: rmse, rsq) [and this is just for the 1st fold, we have 9 more folds]

Let's collect a summary of metrics (across all folds, for each mixture x penalty combination), and plot them.  

Firs, RMSE (lower is better):  

```{r RMSE}

enet_grid_result %>%
  collect_metrics() %>% #this line will come in exam as fill in the blanks #to collect the rmse and rsq across all folds for all combinatinos of mixture and penalty 
  filter(.metric == "rmse") %>% #to filter just the "rmse"
  ggplot(aes(x = penalty, 
             y = mean, 
             color = factor(mixture), #to color based on the factor of mixture
             group = factor(mixture))) +
  geom_line() +
  geom_point() + 
  labs(y = "RMSE")
  
```

We want low value of RMSE (because this is error, so we want a low value of RMSE)

What penalty and mixture values created lowest RMSE?  rewatch video

From the output graph, it looks like having a penalty around 1 with a mixture of 0  gave us the best model (because it has the lowest RMSE). So we should choose this combination (if we choose RMSE as the metric to decide): penalty = 1; mixture = 0.

Now, let's look into R2 (higher is better):  

```{r R2}
enet_grid_result %>%
  collect_metrics() %>%
  filter(.metric == "rsq") %>%
  ggplot(aes(x = penalty, 
             y = mean, 
             color = factor(mixture), 
             group = factor(mixture))) +
  geom_line() +
  geom_point() + 
  labs(y = "R2")
```

[Note: While using R2, the higher the better. From the output graph, the highest R2 we got was for a penalty around 1 and  a mixture of 0.1.

It is important to note that, if we use rmse (RMSE) as the fit metric, it gives us a different model as the best model as compared to rsq (R2) being the fit metric to decide. Normally, we would use/choose one of these metrics (RMSE or R2) to choose the model that we want to go with. Perharps there may a specific reason why we would want to use one over the other. But as long as we explain a given metric and tell them what it is, that should be sufficient to publish a paper. ] 

What penalty and mixture values created lowest RMSE?  
It seems that our best model is with hyperparameters set to:  
  - mixture = 0  
  - penalty = 0  [ask Dr. Bastos: what does he mean here?]
  
Up to this point, we just visually assessed what was the combination of mixture and penalty ("mixture" and "penalty" are the hyperparameters for the elastic net model) that gives us the best predictive model as assessed on our assessment set (notice that we have not used our "test set" yet)

Now, let's extract the hyperparameters from the best model as judged by 2 performance metrics:  
```{r}

# Based on lowest RMSE
best_rmse <- enet_grid_result %>%
  select_best(metric = "rmse")

best_rmse

```
From the output, we see that the best predictive model (that produced the lowest RMSE: Preprocessor1_Model002) has a penalty of 1 and mixture of 0. 

```{r}

# Based on greatest R2
best_r2 <- enet_grid_result %>%
  select_best(metric = "rsq")

best_r2

```

[Why do we get different results each time I clean the environment and rerun all chunks above? Answer: Maybe one of the chunks had some randomness component to it, and was run more than once.] 

From the output, we see that the best predictive model (that produced the highest R2: Preprocessor1_Model013) has a penalty of 1 and mixture of 0.1. 

*Summary:*

- Based on RMSE, we would choose mixture = 0, penalty = 1.
- Based on R2, we would choose mixture = 0.1, penalty = 1.

Very Important Note: Dr. Bastos said that previously he got the following values and we will use these values for the rest of the code chunks for hyperparamters optimization:

- Based on RMSE, we would choose mixture = 0, penalty = 0.
- Based on R2, we would choose mixture = 0, penalty = 0.7.

Let's use the hyperparameter values that optimized R2 to fit our final model.

```{r final_spec}

final_spec <- linear_reg(penalty = 0.7,
                         mixture = 0) %>% #Based on R2, we chose mixture = 0, penalty = 0.7 
  set_engine("glmnet")

final_spec

#Notice that: the piece of code in the above code chunk is completely unaware of any data (training data or test data)

```

  

## 3. Validation  
Now that we determined our best model, let's do our **last fit** (= train the entire data set with the optimal values of "penalty" and "mixture" hyperparameters of elastic net model. Up to this point, the training set has been resampled i.e,. we have not used these "penalty" and "mixture" hyperparameters in the entire training set yet. It was resampled in 10 folds and and we ran those 10 folds which had portion of the training and another part of it was used for assessment. Now, our last fit is gonna take these hyperparameter values and fit the model one last time to the entire training set at once, and then based on that model it's now gonna use the model to predict our test set, to give us the actual predictive ability metrics of this model i.e., actual rsq and rmse on the test set. Notice we are just bringing the test set at this "validation" stage).

This means 2 things:  
  - Traninig the optimum hyperparameter values on the **entire training set**  
  - Using it to **predict** on the **test set**  

These 2 steps can be completed in one function, as below:  

```{r final_fit}

final_fit <- last_fit(final_spec, #specifying the actual data
                      weather_recipe, #specifying the object for recipe
                      split = weather_split #specifying data split
                      )

final_fit %>%
  collect_predictions() #to collect the predictions of the model that was used on the entire training set but getting the predictions on the test set, which we need here to assess the model predictive ability


```

Why 211 rows/observations?  

Because the test set had 211 rows (if we go all the way to the begining, we will see that the test set had 211 rows), and these predictions here are based on the test set. The output contains a ".pred" column which is the prediction of the model for what would be the strength_gtex for the test set, and then we also have a "strength_gtex" column which is the actual observed data for the fiber strength that we brought into the model. So, now we have our observed data (strength_gtex) and our predicted data (.pred) on the test set which has not been seen by the model during training i.e., it was independent of the training set.  

Metrics on the **test set**:  

```{r}

final_fit %>%
  collect_metrics() #to collect rsq and rmse on the test set, because that is what matters -- that's what that tells you the predictive ability of our model

```

From the output, 
On the test set, we have rmse = 3.0295966 and rsq = 0.1322989 (fit metrics of the test set are the most important information to report in a ML paper)

Metrics on **train set** (for curiosity and compare to test set):  

```{r}

# RMSE
final_spec %>%
  fit(strength_gtex ~ .,
      data = bake(weather_prep, 
                  weather_train)) %>%
  augment(new_data = bake(weather_prep, 
                          weather_train)) %>% 
  rmse(strength_gtex, .pred) %>%
  bind_rows(
    
    # R2
    final_spec %>%
      fit(strength_gtex ~ .,
          data = bake(weather_prep, 
                      weather_train)) %>%
      augment(new_data = bake(weather_prep, 
                              weather_train)) %>% 
      rsq(strength_gtex, .pred)
  )

# ?augment

```

From the output, 
On the training set, we have rmse = 2.8547324	 and rsq = 0.1579303	

How does metrics on test compare to metrics on train?  

RMSE:  
- rmse (test set) = 3.0295966
- rmse (training set) = 2.8547324

R2:  
- rsq (test set) = 0.1322989
- rsq (training set) = 0.1579303

The RMSE is smaller in the training set and higher in the test set (the lower RMSE, the better). R2 is higher in the training set and smaller in the test set (the higher R2, the better). So, we observe better fit metrics in the training set (lower RMSE, higher R2) as compared to the test set.

It does not always happen this way, but it is very common to see this. 

Why?

We would expect our model to make better predictions on the training set because it was trained on that. So, it is expected that the fit metrics on the trained set to be better than the fit metrics of the test set, because when we trained the model, it was unaware of the test set data (i.e., it had not seen the data, so it's new information for the model). Since the trained model had not seen the data on the test set, we use the fit metrics on the *test set* to assess performance for the predictive ability of the model.

Regardless of how simple or complex a Machine Learning model is, we should always expect to see a predicted by observed plot, especially on the test set.

Predicted vs. observed plot: only on the test set  

```{r}

final_fit %>%
  collect_predictions() %>%  #will come in exam as fill in the blanks
  ggplot(aes(x = strength_gtex, #will come in exam as fill in the blanks: "strength_gtex" on x
             y = .pred)) + #will come in exam as fill in the blanks: ".pred" on y-axis
  geom_point() +
  geom_abline() + #will come in exam as fill in the blanks #geom_abline(): to add one-to-one line (black line) #something that is very informative in a predicted vs. observed plot is to have this one-to-one line
  geom_smooth(method = "lm") +
  scale_x_continuous(limits = c(20, 40)) +
  scale_y_continuous(limits = c(20, 40)) 

# ?scale_x_continuous # "scale_x_continuous()" and "scale_y_continuous()" : Positions scales for continuous data (x & y

```

**This predicted vs. observed plot for only on the test set.** 
We always want to see **this predicted vs. observed plot for only on the test set** in Machine Learning.

In predicted vs. observed plot (only on the test set), the black line is the one-to-one line. We have strength in the x-axis and the predicted values coming from the model on the y-axis. As we see, maybe the range between 27 - 32 is okayish predicted, but when we have either high or low strength values (on the two tails of the blue fitted line), the model is not doing a good job. What we would like to see if these points (on the blue line) were really close to the (black) one-to-one line, meaning that what was observed was really close to what the model is predicting. In our case, it's okayish. And we were kind of expecting this because based on the R2 (= 0.13) we would not expect a very high correlation.

We always want to see this predicted vs. observed plot for only on the test set in Machine Learning, because giving only RMSE and R2 is good metrics, but this visual assessment understanding is important to have (even though the RMSE and R2 is coming from the blue line, but seeing the plot is more important to understand the shortcomings of the model). 

**Coefficients:**  

```{r}

final_spec %>%
  fit(strength_gtex ~ .,
         data = bake(weather_prep, weather)) %>% #The "bake()" function takes the whole "weather" data set and applies the prepared recipe "weather_prep" to it.
  tidy() %>% #to make the data format tidy i.e., tibble
  arrange(desc(estimate)) #to arrange by the descending order of the "estimate" (= slope)

```

In the output, the "estimate" column gives the slope coefficients of each predictor variables in the model. 

From the output, we see that the mean minimum temperature in September (mean_tmin.c_Sep) had a very high coefficient (estimate = 1.335097e-01) in the model, which probably means that it has a large impact on predicting strength (among all other predictor variables). 

**Variable importance plot:** making a plot of the most important variables

```{r}

final_spec %>%
  fit(strength_gtex ~ .,
         data = bake(weather_prep, weather)) %>%
    vi() %>%  #will come in exam #vi(): to show variable importance (comes from "vip" package)
  mutate(
    Importance = abs(Importance),
    Variable = fct_reorder(Variable, 
                           Importance)
  ) %>%
  slice_head(n = 10) %>%
  ggplot(aes(x = Importance, 
             y = Variable, 
             fill = Sign)) +
  geom_col() + # barchart
  scale_x_continuous(expand = c(0, 0)) +
  labs(y = NULL)
    
```
The Variable importance plot shows the importance of different variables in our model, and whether they had a positive or negative effect on predicting fiber strength. Variable importance plot is also a common plot to see in elastic net machine learning model.

**Therefore, tmin in September (positive effect), and tmax in July (negative effect) were the most important variables affecting cotton fiber strength.**  

Mean minimum temperature in September and May had a positive effect on the fiber strength i.e., If there was an increase in the minimum temperature in September and May (on the average), there was a positive effect on fiber strength. 

The maximum temperature in July and September having a negative effect i.e., if there was a increase on the maximum temperature in July and September (on the average), there was a negative effect on fiber strength. 

# Summary  
In this exercise, we covered: 
  - Penalized linear regression types (ridge, lasso, elastic net)  
  - Set up a ML workflow to train an elastic net model  
  - Used `recipes` to process data
  - Used `rsamples` to split data  
  - Used a fixed grid to search the best values for mixture and penalty  
  - Used 10-fold cross validation as the resampling method  
  - Used both R2 and RMSE as the metrics (on the training data only; on the different folds of the training data) to select best model  
  - Once final model was determined (Once the best hyperparameter values were determined), used it to predict **test set**  [notice that we used the test set only at the end]
  - Evaluated the model [on the test set] with predicted vs. observed plot, R2 and RMSE fit metrics, and variable importance  
  

